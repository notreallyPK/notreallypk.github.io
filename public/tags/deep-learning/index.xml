<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on Pratik Kokil</title>
    <link>http://localhost:58927/tags/deep-learning/</link>
    <description>Recent content in Deep Learning on Pratik Kokil</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Aug 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:58927/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Exploring SAM2 and Scene Graphs</title>
      <link>http://localhost:58927/notes/exploring-scene-graphs/</link>
      <pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:58927/notes/exploring-scene-graphs/</guid>
      <description>&lt;p&gt;I recently came across an idea, in which scene-graphs are used to describe what is happening in a given video frame, below is a very short demonstration along with a link to colab notebook where I explore using Meta&amp;rsquo;s Segment Anything 2 (SAM2) model to create scene graphs.&lt;/p&gt;&#xA;&lt;p&gt;The video we are going to be using is as follows, we want SAM2 to track the cup that contains the ball and in the end tell us the correct position.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Audio Transcription Tool</title>
      <link>http://localhost:58927/projects/yatt/</link>
      <pubDate>Sun, 13 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:58927/projects/yatt/</guid>
      <description>&lt;p&gt;Working on learning about &lt;a href=&#34;https://en.wikipedia.org/wiki/Speech_recognition&#34;&gt;ASR&lt;/a&gt; and shipping something cool.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reader - Text To Speech Tool</title>
      <link>http://localhost:58927/projects/read-aloud/</link>
      <pubDate>Sun, 13 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:58927/projects/read-aloud/</guid>
      <description>&lt;p&gt;Pocket is dead, and speechify costs money. A project to explore if we can run TTS models locally on a phone, and create a great reading experience.&lt;/p&gt;&#xA;&lt;p&gt;Track notes and status &lt;a href=&#34;http://localhost:58927/content/notes/exploring-tts-one.md&#34;&gt;starting here&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sensor data fusion: Camera &amp; Radar</title>
      <link>http://localhost:58927/projects/sensor-fusion/</link>
      <pubDate>Mon, 10 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:58927/projects/sensor-fusion/</guid>
      <description>&lt;p&gt;This was a team project done under supervision of &lt;a href=&#34;https://www.thi.de/&#34;&gt;Technische Hochschule Ingolstadt&lt;/a&gt;. The goal was to perform late-fusion on radar &amp;amp; camerea detections.&lt;/p&gt;&#xA;&lt;p&gt;This was really fun to work on - got to learn about nitty-gritties of model training, evaluation and spatial fusion of detections. I would like to highlight a few sections here from the project.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nairjayesh/Sensor-Data-Fusion&#34;&gt;Github Repo&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;image-augmentation&#34;&gt;Image Augmentation:&lt;/h3&gt;&#xA;&lt;p&gt;Primary problem when it comes to working on road user data is high levels of data imbalance among different classes, we were working with 6  road user classes with the following distribution.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring Reinforcement Learning</title>
      <link>http://localhost:58927/projects/exploring-rl/</link>
      <pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:58927/projects/exploring-rl/</guid>
      <description>&lt;p&gt;This was initially a project done as part of Principles of Autonomy and Decision Making at &lt;a href=&#34;https://www.thi.de/&#34;&gt;Technische Hochschule Ingolstadt&lt;/a&gt;. The goal was to implement and test Q-Learning and Deep-Q-Learning in a custom environment.&lt;/p&gt;&#xA;&lt;p&gt;Currently, this page serves as a gateway to exploring different RL algorithms on various environments, starting with Value function based methods.&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-q-learning-and-deep-q-learning&#34;&gt;1/ Q-Learning and Deep Q-Learning&lt;/h2&gt;&#xA;&lt;h3 id=&#34;11-the-environment&#34;&gt;1.1/ The Environment:&lt;/h3&gt;&#xA;&lt;p&gt;The custom environment was built on top of frameworks such as Gymnasium and PyGames. It essentially, involved an agent (Pikachu) who had to navigate it&amp;rsquo;s way through a maze toward&amp;rsquo;s the goal state (Ash) while collecting reward&amp;rsquo;s (badges) and avoiding terminal state&amp;rsquo;s (Meowth).&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
